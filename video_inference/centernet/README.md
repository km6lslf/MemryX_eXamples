# Object Detection Using Centernet

The **CeneterNet** example demonstrates real-time object detection on a single input stream using the pre-trained centernet_mobilenetv2_fpn_kpts model on MemryX accelerators. This guide provides setup instructions, model details, and necessary code snippets to help you quickly get started.

<p align="center">
  <img src="assets/centernet.gif" alt="CenterNet Example" width="35%" />
</p>

## Overview

| Property             | Details                                                                 |
|----------------------|-------------------------------------------------------------------------|
| **Model**            | [CenterNet](https://arxiv.org/abs/1904.07850)                                            |
| **Model Type**       | Object Detection                                                      |
| **Framework**        | [onnx](https://onnx.ai/),[tensorflow, tflite](https://www.tensorflow.org/)                                                  |
| **Model Source**     | [Download](http://download.tensorflow.org/models/object_detection/tf2/20210210/centernet_mobilenetv2fpn_512x512_coco17_kpts.tar.gz) |
| **Pre-compiled DFP** | [Download here](https://developer.memryx.com/docs/model_explorer/CenterNet-MobileNet-v2-kpts_320_320_3.zip)                                           |
| **Dataset**          | [COCO](https://cocodataset.org/#home) |
| **Model Resolution**            | 320x320                                                    |
| **Output**           | Bounding box coordinates with object probabilities |
| **OS**               | Linux |
| **License**          | [MIT](LICENSE.md)                                         |

## Requirements (Linux)

Before running the application ensure that all MemryX runtime plugins and utilities libraries are installed. For more information on installation, please refer to DevHub pages such as [runtime installation page](https://developer.memryx.com/docs_dev/get_started/install_driver.html) , and [additional requirements for tutorial apps page](https://developer.memryx.com/docs_dev/tutorials/requirements/installation.html)

## Running the Application (Linux)

### Step 1: Download Pre-compiled DFP

To download and unzip the precompiled DFPs, use the following commands:
```bash
cd assets
wget https://developer.memryx.com/example_files/centernet.zip
mkdir -p models
unzip centernet.zip -d models
```

<details> 
<summary> (Optional) Download and compile the model yourself </summary>
This step is optional if the pre-compiled dfp is downloaded from the link provided above.

Download the pretrained Centernet from the source github.

```bash
wget http://download.tensorflow.org/models/object_detection/tf2/20210210/centernet_mobilenetv2fpn_512x512_coco17_kpts.tar.gz
tar -xvf centernet_mobilenetv2fpn_512x512_coco17_kpts.tar.gz
cd centernet_mobilenetv2_fpn_kpts
python -m tf2onnx.convert --saved-model saved_model --output centernet.onnx --verbose --opset 18

```
The export script will generate a centernet.onnx and model.tflite files.

You can use the MemryX Neural Compiler to compile the model and generate the DFP file required by the accelerator. If you prefer, you can download the pre-compiled DFP and skip this step.

```bash
 mx_nc -m centernet.onnx -v --autocrop --dfp_fname centernet_onnx
 mx_nc -m saved_model/saved_model.pb -v --autocrop --dfp_fname centernet_tf
 mx_nc -m model.tflite -v --autocrop --dfp_fname centernet_tflite
```
The compiler will generate the DFP, a pre-processing and a post-processing file which can be passed as inputs to the application.

</details>

### Step 2: Run the Script/Program

With the compiled model, you can now run real-time inference. Below are the examples of how to do this using C++.

#### C++

To run the C++ example for object detection with Centernet using MX3, simply execute the following steps:

1. Build the project using CMake. From the project directory, execute:

```bash
# ensure a camera device is connected as default video input is a cam
cd src/cpp
mkdir build && cd build
cmake ..
make -j
```

2. Run the application.

You need to specify whether you want to use the camera or a video file as input. You also need to specify the runtime framework needed to run the pre and post processing.

* To run the application using the default DFP file and a camera as input, use the following command:

```bash
./CenterNet onnx cam:<cam index>
./CenterNet tf cam:<cam index>
./CenterNet tflite cam:<cam index>
```

* To run the application using the default DFP file and a video file as input, use the following command:

```bash
./CenterNet onnx vid:<video file>
./CenterNet tf vid:<video file>
./CenterNet tflite vid:<video file>
```

## Tutorial

A more detailed tutorial with complete code explanations is available on the [MemryX Developer Hub](https://developer.memryx.com). You can find it [here](https://developer.memryx.com/docs/tutorials/realtime_inf/autocrop_inf/autocrop_centernet.html)


## Third-Party License

This project uses third-party software and libraries. Below are the details of the licenses for these dependencies:

- **Model**: Copyright (c) Google Inc., [Apache License](https://github.com/tensorflow/models/blob/master/LICENSE)

- **Code and Pre/Post-Processing**: Some code components, including pre/post-processing, were sourced from the CenterNet model provided on [TFModelZoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) ðŸ”—

- **Preview Video**: ["People walking in and out of a store" on Pexels](https://www.pexels.com/video/people-going-inside-a-store-with-automatic-sliding-doors-6641527/)  
  - License: [Pexels License](https://www.pexels.com/license/)

## Summary

This guide offers a quick and easy way to run single stream object detection using the CenterNet model on MemryX accelerators. Download the full code and the pre-compiled DFP file to get started immediately.
